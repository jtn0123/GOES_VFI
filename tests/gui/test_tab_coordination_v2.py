"""Optimized tab-specific functionality and coordination tests for GOES VFI GUI.

Optimizations applied:
- Mock-based testing to avoid GUI dependencies and segfaults
- Shared fixtures for common components and test data
- Parameterized scenarios for comprehensive tab coordination coverage
- Enhanced error handling and state management validation
- Streamlined tab interaction simulation
"""

from typing import Any
from unittest.mock import MagicMock

from PyQt6.QtCore import QObject, QThread, pyqtSignal
from PyQt6.QtWidgets import (
    QApplication,
    QLabel,
    QListWidget,
    QMessageBox,
    QProgressBar,
    QPushButton,
    QTreeWidget,
)
import pytest


class MockModelDownloader(QThread):
    """Mock model downloader for testing."""

    progress = pyqtSignal(str, int)  # model_name, percent
    finished = pyqtSignal(str, bool, str)  # model_name, success, message

    def __init__(self, model_name: Any, model_size_mb: Any = 100) -> None:
        super().__init__()
        self.model_name = model_name
        self.model_size_mb = model_size_mb
        self.cancelled = False

    def run(self) -> None:
        """Simulate model download with faster execution for testing."""
        for i in range(0, 101, 25):  # Reduced iterations for faster testing
            if self.cancelled:
                self.finished.emit(self.model_name, False, "Cancelled")  # noqa: FBT003
                return
            self.progress.emit(self.model_name, i)
            self.msleep(10)  # Reduced sleep time

        self.finished.emit(self.model_name, True, "Download complete")  # noqa: FBT003


class TestTabCoordinationV2:
    """Optimized test class for tab-specific functionality and inter-tab coordination."""

    @pytest.fixture(scope="class")
    @staticmethod
    def shared_app() -> Any:
        """Create shared QApplication for tests.

        Returns:
            QApplication: The shared Qt application instance.
        """
        app = QApplication.instance()
        if app is None:
            app = QApplication([])
        return app

    @pytest.fixture()
    @staticmethod
    def mock_main_window(shared_app: Any) -> Any:  # noqa: ARG004
        """Create mock MainWindow with essential tab components.

        Returns:
            Any: Mock MainWindow instance with tab components.
        """
        window = MagicMock()

        # Mock tab widget
        window.tab_widget = MagicMock()
        window.tab_widget.count = MagicMock(return_value=3)
        window.tab_widget.addTab = MagicMock(return_value=3)
        window.tab_widget.removeTab = MagicMock()
        window.tab_widget.tabBar = MagicMock()

        # Mock main tab components
        window.main_tab = MagicMock()
        window.integrity_tab = MagicMock()
        window.models_tab = MagicMock()

        # Mock common methods
        window._post_init_setup = MagicMock()  # noqa: SLF001

        return window

    @pytest.fixture()
    @staticmethod
    def mock_model_library_tab(shared_app: Any) -> Any:  # noqa: ARG004
        """Create mock model library tab.

        Returns:
            Any: Mock model library tab instance.
        """
        tab = MagicMock()

        # Mock UI components
        tab.model_list = MagicMock(spec=QListWidget)
        tab.download_button = MagicMock(spec=QPushButton)
        tab.delete_button = MagicMock(spec=QPushButton)
        tab.progress_bars = {}
        tab.active_downloads = {}

        # Mock available models
        tab.available_models = {
            "rife-v4.6": {"size": 150, "installed": False},
            "rife-v4.13": {"size": 180, "installed": False},
            "rife-v4.14": {"size": 200, "installed": True},
        }

        return tab

    @staticmethod
    def test_model_library_download_operations(mock_model_library_tab: Any) -> None:  # noqa: C901
        """Test model library download operations."""
        tab = mock_model_library_tab

        # Mock model library tab behavior
        class ModelLibraryManager:
            def __init__(self, tab: Any) -> None:
                self.tab = tab

            def populate_model_list(self) -> None:
                self.tab.model_list.clear()
                items = []
                for model_name, info in self.tab.available_models.items():
                    item_text = f"{model_name} ({info['size']}MB)"
                    if info["installed"]:
                        item_text += " âœ“"
                    items.append((item_text, model_name))
                self.tab.model_list.items = items
                self.tab.model_list.count.return_value = len(items)

            def download_selected_model(self, model_name: Any) -> bool:
                if model_name in self.tab.active_downloads:
                    return False  # Already downloading

                # Create mock progress bar
                progress_bar = MagicMock(spec=QProgressBar)
                self.tab.progress_bars[model_name] = progress_bar

                # Create mock downloader
                downloader = MockModelDownloader(model_name)
                self.tab.active_downloads[model_name] = downloader

                return True

            def update_download_progress(self, model_name: Any, percent: Any) -> None:
                if model_name in self.tab.progress_bars:
                    self.tab.progress_bars[model_name].setValue(percent)

            def handle_download_finished(self, model_name: Any, success: Any, message: Any) -> None:  # noqa: ARG002
                if success:
                    self.tab.available_models[model_name]["installed"] = True

                # Clean up
                if model_name in self.tab.active_downloads:
                    del self.tab.active_downloads[model_name]
                if model_name in self.tab.progress_bars:
                    del self.tab.progress_bars[model_name]

        # Test download operations
        manager = ModelLibraryManager(tab)
        manager.populate_model_list()

        # Verify initial state
        assert tab.model_list.count() == 3
        assert not tab.available_models["rife-v4.6"]["installed"]

        # Start download
        success = manager.download_selected_model("rife-v4.6")
        assert success
        assert "rife-v4.6" in tab.active_downloads
        assert "rife-v4.6" in tab.progress_bars

        # Simulate progress updates
        manager.update_download_progress("rife-v4.6", 50)
        tab.progress_bars["rife-v4.6"].setValue.assert_called_with(50)

        # Simulate download completion
        manager.handle_download_finished("rife-v4.6", True, "Download complete")  # noqa: FBT003
        assert tab.available_models["rife-v4.6"]["installed"]
        assert "rife-v4.6" not in tab.active_downloads

    @pytest.mark.parametrize(
        "user_choice,expected_deleted",
        [
            (QMessageBox.StandardButton.Yes, True),
            (QMessageBox.StandardButton.No, False),
        ],
    )
    @staticmethod
    def test_model_library_delete_operations(
        mock_model_library_tab: Any,  # noqa: ARG004
        user_choice: Any,
        expected_deleted: Any,
    ) -> None:
        """Test model library delete operations with user confirmation."""

        # Mock model manager
        class MockModelManager:
            def __init__(self) -> None:
                self.installed_models = {
                    "rife-v4.6": "/models/rife-v4.6",
                    "rife-v4.13": "/models/rife-v4.13",
                }

            def delete_model(self, model_name: Any, confirm_func: Any) -> bool:
                if model_name in self.installed_models:
                    # Use provided confirmation function
                    result = confirm_func(f"Are you sure you want to delete {model_name}?")

                    if result == QMessageBox.StandardButton.Yes:
                        del self.installed_models[model_name]
                        return True
                return False

            @staticmethod
            def get_model_size(model_name: Any) -> Any:
                sizes = {
                    "rife-v4.6": 150 * 1024 * 1024,
                    "rife-v4.13": 180 * 1024 * 1024,
                }
                return sizes.get(model_name, 0)

        # Mock confirmation dialog
        def mock_confirm(message: Any) -> Any:  # noqa: ARG001
            return user_choice

        # Test deletion
        manager = MockModelManager()
        initial_count = len(manager.installed_models)

        success = manager.delete_model("rife-v4.6", mock_confirm)

        if expected_deleted:
            assert success
            assert len(manager.installed_models) == initial_count - 1
            assert "rife-v4.6" not in manager.installed_models
        else:
            assert not success
            assert len(manager.installed_models) == initial_count
            assert "rife-v4.6" in manager.installed_models

    @staticmethod
    def test_integrity_check_scan_workflow(shared_app: Any) -> None:  # noqa: ARG004, C901
        """Test integrity check scan workflow."""

        # Mock integrity scanner
        class MockIntegrityScanner(QObject):
            scan_progress = pyqtSignal(int, str)
            scan_complete = pyqtSignal(dict)

            def __init__(self) -> None:
                super().__init__()
                self.is_scanning = False
                self.scan_results = {}

            def start_scan(self, directory: Any) -> None:
                self.is_scanning = True
                self.scan_results = {
                    "total_files": 0,
                    "corrupted_files": [],
                    "missing_files": [],
                    "valid_files": [],
                }

                # Simulate immediate scan for testing
                self._perform_scan(directory)

            def _perform_scan(self, directory: Any) -> None:  # noqa: ARG002
                # Mock scanning files
                test_files = [
                    ("file1.nc", "valid"),
                    ("file2.nc", "corrupted"),
                    ("file3.nc", "valid"),
                    ("file4.nc", "missing"),
                ]

                total = len(test_files)
                for i, (filename, status) in enumerate(test_files):
                    if not self.is_scanning:
                        break

                    progress = int((i + 1) / total * 100)
                    self.scan_progress.emit(progress, f"Scanning {filename}")

                    self.scan_results["total_files"] += 1
                    if status == "valid":
                        self.scan_results["valid_files"].append(filename)
                    elif status == "corrupted":
                        self.scan_results["corrupted_files"].append(filename)
                    elif status == "missing":
                        self.scan_results["missing_files"].append(filename)

                self.is_scanning = False
                self.scan_complete.emit(self.scan_results)

        # Create scanner and mock UI components
        scanner = MockIntegrityScanner()
        progress_label = MagicMock(spec=QLabel)
        results_tree = MagicMock(spec=QTreeWidget)

        # Track progress and results
        progress_updates = []
        final_results = None

        def on_progress(percent: Any, message: Any) -> None:
            progress_updates.append((percent, message))
            progress_label.setText(message)

        def on_complete(results: Any) -> None:
            nonlocal final_results
            final_results = results
            # Mock tree population
            results_tree.clear()
            results_tree.topLevelItemCount.return_value = 3

        # Connect signals
        scanner.scan_progress.connect(on_progress)
        scanner.scan_complete.connect(on_complete)

        # Start scan
        scanner.start_scan("/test/data")

        # Verify scan results
        assert not scanner.is_scanning
        assert len(progress_updates) == 4  # One for each file
        assert final_results is not None
        assert final_results["total_files"] == 4
        assert len(final_results["valid_files"]) == 2
        assert len(final_results["corrupted_files"]) == 1
        assert len(final_results["missing_files"]) == 1

    @pytest.mark.parametrize(
        "file_type,repair_success",
        [
            ("corrupted", True),
            ("missing", True),
            ("unrepairable", False),
        ],
    )
    @staticmethod
    def test_integrity_check_repair_actions(shared_app: Any, file_type: Any, repair_success: Any) -> None:  # noqa: ARG004, C901
        """Test integrity check repair actions with different scenarios."""

        # Mock repair manager
        class MockRepairManager:
            def __init__(self) -> None:
                self.repair_queue = []
                self.repair_results = {}

            def queue_repair(self, file_info: Any) -> None:
                self.repair_queue.append(file_info)

            def repair_corrupted_file(self, filepath: Any) -> Any:
                success = "unrepairable" not in filepath
                if success:
                    self.repair_results[filepath] = "Repaired successfully"
                else:
                    self.repair_results[filepath] = "Repair failed - file too damaged"
                return success

            def download_missing_file(self, filename: Any, source_url: Any) -> bool:
                self.repair_results[filename] = f"Downloaded from {source_url}"
                return True

            def process_repair_queue(self, progress_callback: Any) -> None:
                total = len(self.repair_queue)
                for i, file_info in enumerate(self.repair_queue):
                    progress = int((i + 1) / total * 100)
                    progress_callback(progress, f"Repairing {file_info['name']}")

                    if file_info["type"] == "corrupted":
                        self.repair_corrupted_file(file_info["path"])
                    elif file_info["type"] == "missing":
                        self.download_missing_file(file_info["name"], file_info["url"])

                self.repair_queue.clear()

        # Test repair scenario
        repair_mgr = MockRepairManager()

        # Create file info based on test parameters
        if file_type == "corrupted":
            file_info = {"name": "corrupted.nc", "path": "/data/corrupted.nc", "type": "corrupted"}
        elif file_type == "missing":
            file_info = {
                "name": "missing.nc",
                "path": "/data/missing.nc",
                "type": "missing",
                "url": "http://example.com/missing.nc",
            }
        else:  # unrepairable
            file_info = {"name": "unrepairable.nc", "path": "unrepairable.nc", "type": "corrupted"}

        # Queue and process repair
        repair_mgr.queue_repair(file_info)

        repair_progress = []
        repair_mgr.process_repair_queue(lambda p, m: repair_progress.append((p, m)))

        # Verify results
        assert len(repair_mgr.repair_results) == 1

        if file_type == "corrupted" and repair_success:
            assert "Repaired successfully" in repair_mgr.repair_results[file_info["path"]]
        elif file_type == "missing":
            assert "Downloaded from" in repair_mgr.repair_results[file_info["name"]]
        elif not repair_success:
            assert "Repair failed" in repair_mgr.repair_results[file_info["path"]]

    @pytest.mark.parametrize(
        "setting_name,value,expected_valid",
        [
            ("thread_count", "8", True),
            ("thread_count", "50", False),  # Too high
            ("tile_size", "256", True),
            ("tile_size", "50", False),  # Too low
            ("memory_limit", "0.5", True),
            ("memory_limit", "1.5", False),  # Too high
            ("invalid_setting", "any_value", True),  # Unknown settings pass through
        ],
    )
    @staticmethod
    def test_advanced_settings_validation(shared_app: Any, setting_name: Any, value: Any, expected_valid: Any) -> None:  # noqa: ARG004, C901
        """Test advanced settings validation with various scenarios."""

        # Mock settings validator
        class MockSettingsValidator:
            def __init__(self) -> None:
                self.validation_rules = {
                    "thread_count": (1, 32, int),
                    "tile_size": (128, 1024, int),
                    "memory_limit": (0.1, 0.9, float),
                    "cache_size": (100, 10000, int),
                    "network_timeout": (5, 300, int),
                }

            def validate_setting(self, name: Any, value: Any) -> Any:
                if name not in self.validation_rules:
                    return True, value

                min_val, max_val, type_func = self.validation_rules[name]

                try:
                    typed_value = type_func(value)
                except (ValueError, TypeError):
                    return False, f"Invalid {type_func.__name__} value"
                else:
                    if min_val <= typed_value <= max_val:
                        return True, typed_value
                    return False, f"Value must be between {min_val} and {max_val}"

            def validate_all_settings(self, settings_dict: Any) -> Any:
                errors = []
                validated = {}

                for name, setting_value in settings_dict.items():
                    valid, result = self.validate_setting(name, setting_value)
                    if valid:
                        validated[name] = result
                    else:
                        errors.append(f"{name}: {result}")

                return validated, errors

        # Test single setting validation
        validator = MockSettingsValidator()
        valid, result = validator.validate_setting(setting_name, value)

        assert valid == expected_valid

        if expected_valid:
            if setting_name in validator.validation_rules:
                expected_type = validator.validation_rules[setting_name][2]
                assert isinstance(result, expected_type)
        else:
            assert isinstance(result, str)  # Error message

    @staticmethod
    def test_inter_tab_state_synchronization(shared_app: Any) -> None:  # noqa: ARG004
        """Test state synchronization between tabs."""

        # Mock tab state synchronizer
        class MockTabStateSynchronizer(QObject):
            state_changed = pyqtSignal(str, object)

            def __init__(self) -> None:
                super().__init__()
                self.shared_state = {}
                self.tab_subscriptions = {}

            def register_tab(self, tab_name: Any, keys_of_interest: Any) -> None:
                self.tab_subscriptions[tab_name] = keys_of_interest

            def update_state(self, key: Any, value: Any) -> None:
                old_value = self.shared_state.get(key)
                if old_value != value:
                    self.shared_state[key] = value
                    self.state_changed.emit(key, value)

            def get_state(self, key: Any, default: Any = None) -> Any:
                return self.shared_state.get(key, default)

            def sync_to_tab(self, tab_name: Any, update_func: Any) -> None:
                if tab_name in self.tab_subscriptions:
                    for key in self.tab_subscriptions[tab_name]:
                        if key in self.shared_state:
                            update_func(key, self.shared_state[key])

        # Create synchronizer
        sync = MockTabStateSynchronizer()

        # Register tabs with their interests
        sync.register_tab("main", ["input_dir", "output_file", "processing"])
        sync.register_tab("settings", ["encoder", "fps", "quality"])
        sync.register_tab("models", ["current_model", "available_models"])

        # Track state changes
        state_changes = []
        sync.state_changed.connect(lambda k, v: state_changes.append((k, v)))

        # Update states
        sync.update_state("input_dir", "/test/input")
        sync.update_state("encoder", "RIFE")
        sync.update_state("current_model", "rife-v4.6")

        # Verify state changes were emitted
        assert len(state_changes) == 3
        assert ("input_dir", "/test/input") in state_changes
        assert ("encoder", "RIFE") in state_changes
        assert ("current_model", "rife-v4.6") in state_changes

        # Test tab synchronization
        main_tab_updates = {}
        settings_tab_updates = {}

        sync.sync_to_tab("main", lambda k, v: main_tab_updates.update({k: v}))
        sync.sync_to_tab("settings", lambda k, v: settings_tab_updates.update({k: v}))

        # Verify correct subscription filtering
        assert "input_dir" in main_tab_updates
        assert "encoder" not in main_tab_updates  # Not subscribed

        assert "encoder" in settings_tab_updates
        assert "input_dir" not in settings_tab_updates  # Not subscribed

    @staticmethod
    def test_dynamic_tab_management(mock_main_window: Any) -> None:
        """Test dynamic tab creation and removal."""
        window = mock_main_window

        # Mock dynamic tab manager
        class MockDynamicTabManager:
            def __init__(self, tab_widget: Any) -> None:
                self.tab_widget = tab_widget
                self.dynamic_tabs = {}
                self.next_index = 3  # Assume 3 static tabs exist

            def add_custom_tab(self, widget: Any, title: Any, *, closable: bool = True) -> Any:
                index = self.next_index
                self.next_index += 1

                # Mock adding tab
                self.tab_widget.addTab(widget, title)
                self.tab_widget.addTab.return_value = index

                self.dynamic_tabs[title] = {
                    "widget": widget,
                    "index": index,
                    "closable": closable,
                }

                return index

            def remove_tab(self, index: Any) -> None:
                # Find tab by index
                for title, info in list(self.dynamic_tabs.items()):
                    if info["index"] == index:
                        self.tab_widget.removeTab(index)
                        del self.dynamic_tabs[title]

                        # Update indices for tabs after removed one
                        for other_info in self.dynamic_tabs.values():
                            if other_info["index"] > index:
                                other_info["index"] -= 1
                        break

            def get_tab(self, title: Any) -> Any:
                return self.dynamic_tabs.get(title)

        # Test dynamic tab management
        tab_mgr = MockDynamicTabManager(window.tab_widget)

        # Add first dynamic tab
        custom_widget = MagicMock(spec=QListWidget)
        index1 = tab_mgr.add_custom_tab(custom_widget, "Analysis Results")

        assert "Analysis Results" in tab_mgr.dynamic_tabs
        assert tab_mgr.dynamic_tabs["Analysis Results"]["index"] == index1
        window.tab_widget.addTab.assert_called_with(custom_widget, "Analysis Results")

        # Add second dynamic tab
        log_widget = MagicMock(spec=QListWidget)
        index2 = tab_mgr.add_custom_tab(log_widget, "Process Log", closable=False)

        assert "Process Log" in tab_mgr.dynamic_tabs
        assert not tab_mgr.dynamic_tabs["Process Log"]["closable"]

        # Remove first tab
        tab_mgr.remove_tab(index1)

        assert "Analysis Results" not in tab_mgr.dynamic_tabs
        assert "Process Log" in tab_mgr.dynamic_tabs
        window.tab_widget.removeTab.assert_called_with(index1)

        # Verify second tab index was updated
        assert tab_mgr.dynamic_tabs["Process Log"]["index"] == index2 - 1

    @pytest.mark.parametrize(
        "active_tab,shortcut,should_trigger",
        [
            ("main", "Ctrl+R", True),
            ("main", "Ctrl+D", False),  # Wrong tab
            ("models", "Ctrl+D", True),
            ("models", "Ctrl+R", False),  # Wrong tab
            ("any", "F1", True),  # Global shortcut
        ],
    )
    @staticmethod
    def test_tab_specific_shortcuts(shared_app: Any, active_tab: Any, shortcut: Any, should_trigger: Any) -> None:  # noqa: ARG004
        """Test tab-specific keyboard shortcuts."""

        # Mock shortcut manager
        class MockTabShortcutManager:
            def __init__(self) -> None:
                self.shortcuts = {}
                self.active_tab = None
                self.triggered_actions = []

            def register_shortcut(self, tab_name: Any, key_sequence: Any, action: Any) -> None:
                if tab_name not in self.shortcuts:
                    self.shortcuts[tab_name] = {}
                self.shortcuts[tab_name][key_sequence] = action

            def set_active_tab(self, tab_name: Any) -> None:
                self.active_tab = tab_name

            def handle_shortcut(self, key_sequence: Any) -> bool:
                # Check tab-specific shortcuts first
                if (
                    self.active_tab
                    and self.active_tab in self.shortcuts
                    and key_sequence in self.shortcuts[self.active_tab]
                ):
                    action = self.shortcuts[self.active_tab][key_sequence]
                    action()
                    return True

                # Check global shortcuts
                if "global" in self.shortcuts and key_sequence in self.shortcuts["global"]:
                    self.shortcuts["global"][key_sequence]()
                    return True

                return False

        # Create manager and register shortcuts
        shortcut_mgr = MockTabShortcutManager()

        # Register shortcuts with action tracking
        shortcut_mgr.register_shortcut("main", "Ctrl+R", lambda: shortcut_mgr.triggered_actions.append("refresh_main"))
        shortcut_mgr.register_shortcut(
            "models", "Ctrl+D", lambda: shortcut_mgr.triggered_actions.append("download_model")
        )
        shortcut_mgr.register_shortcut("global", "F1", lambda: shortcut_mgr.triggered_actions.append("show_help"))

        # Set active tab
        if active_tab != "any":
            shortcut_mgr.set_active_tab(active_tab)
        else:
            shortcut_mgr.set_active_tab("main")  # Default for global test

        # Test shortcut handling
        initial_count = len(shortcut_mgr.triggered_actions)
        result = shortcut_mgr.handle_shortcut(shortcut)

        if should_trigger:
            assert result
            assert len(shortcut_mgr.triggered_actions) == initial_count + 1
        else:
            assert not result
            assert len(shortcut_mgr.triggered_actions) == initial_count

    @staticmethod
    def test_tab_data_persistence(shared_app: Any) -> None:  # noqa: ARG004
        """Test tab data persistence and restoration."""

        # Mock tab data manager
        class MockTabDataManager:
            def __init__(self) -> None:
                self.tab_data = {}

            def save_tab_state(self, tab_name: Any, state_data: Any) -> None:
                self.tab_data[tab_name] = state_data.copy()

            def restore_tab_state(self, tab_name: Any) -> Any:
                return self.tab_data.get(tab_name, {})

            def clear_tab_state(self, tab_name: Any) -> None:
                if tab_name in self.tab_data:
                    del self.tab_data[tab_name]

            def get_all_tab_states(self) -> Any:
                return self.tab_data.copy()

        # Test data persistence
        data_mgr = MockTabDataManager()

        # Save state for different tabs
        main_state = {
            "input_dir": "/test/input",
            "output_file": "/test/output.mp4",
            "last_settings": {"fps": 30, "encoder": "RIFE"},
        }

        models_state = {"selected_model": "rife-v4.6", "download_queue": ["rife-v4.13", "rife-v4.14"]}

        data_mgr.save_tab_state("main", main_state)
        data_mgr.save_tab_state("models", models_state)

        # Verify states were saved
        assert len(data_mgr.tab_data) == 2

        # Restore and verify
        restored_main = data_mgr.restore_tab_state("main")
        restored_models = data_mgr.restore_tab_state("models")

        assert restored_main == main_state
        assert restored_models == models_state

        # Test non-existent tab
        empty_state = data_mgr.restore_tab_state("non_existent")
        assert empty_state == {}

        # Clear state
        data_mgr.clear_tab_state("main")
        assert "main" not in data_mgr.tab_data
        assert "models" in data_mgr.tab_data

    @staticmethod
    def test_tab_error_handling(shared_app: Any) -> None:  # noqa: ARG004, C901
        """Test error handling across different tabs."""

        # Mock tab error handler
        class MockTabErrorHandler:
            def __init__(self) -> None:
                self.error_history = []
                self.recovery_actions = []

            def handle_tab_error(self, tab_name: Any, error_type: Any, error_message: Any) -> Any:
                error_info = {
                    "tab": tab_name,
                    "type": error_type,
                    "message": error_message,
                    "timestamp": "mock_timestamp",
                }
                self.error_history.append(error_info)

                # Determine recovery action
                if error_type == "network_error":
                    recovery_action = "retry_with_timeout"
                elif error_type == "file_permission":
                    recovery_action = "request_permissions"
                elif error_type == "memory_error":
                    recovery_action = "reduce_memory_usage"
                elif error_type == "validation_error":
                    recovery_action = "show_validation_dialog"
                else:
                    recovery_action = "show_generic_error"

                self.recovery_actions.append(recovery_action)
                return recovery_action

            def get_error_count_for_tab(self, tab_name: Any) -> Any:
                return len([e for e in self.error_history if e["tab"] == tab_name])

            def clear_error_history(self) -> None:
                self.error_history.clear()
                self.recovery_actions.clear()

        # Test error handling scenarios
        error_handler = MockTabErrorHandler()

        # Test different error types
        error_scenarios = [
            ("main", "network_error", "Failed to connect to server"),
            ("models", "file_permission", "Permission denied accessing model files"),
            ("integrity", "memory_error", "Insufficient memory for scan"),
            ("settings", "validation_error", "Invalid configuration values"),
        ]

        for tab_name, error_type, error_message in error_scenarios:
            recovery_action = error_handler.handle_tab_error(tab_name, error_type, error_message)

            # Verify error was recorded
            assert error_handler.get_error_count_for_tab(tab_name) > 0

            # Verify appropriate recovery action
            if error_type == "network_error":
                assert recovery_action == "retry_with_timeout"
            elif error_type == "file_permission":
                assert recovery_action == "request_permissions"
            elif error_type == "memory_error":
                assert recovery_action == "reduce_memory_usage"
            elif error_type == "validation_error":
                assert recovery_action == "show_validation_dialog"

        # Verify all errors were recorded
        assert len(error_handler.error_history) == len(error_scenarios)
        assert len(error_handler.recovery_actions) == len(error_scenarios)
