"""
Optimized unit tests for run_vfi parameterized functionality with maintained coverage.

This v2 version maintains all test scenarios while optimizing through:
- Shared fixtures for VFI pipeline setup and mock configurations
- Combined VFI processing testing scenarios for different execution paths
- Batch validation of RIFE and FFmpeg command execution
- Enhanced error handling and edge case coverage
"""

import pathlib
import subprocess
from typing import Any, Dict, List
from unittest.mock import MagicMock, patch

import pytest

from goesvfi.pipeline import run_vfi as run_vfi_mod
from tests.utils.mocks import (
    create_mock_colourise,
    create_mock_popen,
    create_mock_subprocess_run,
)


class TestRunVfiParamOptimizedV2:
    """Optimized VFI parameterized tests with full coverage."""

    @pytest.fixture(scope="class")
    def vfi_test_components(self):
        """Create shared components for VFI testing."""
        
        # Enhanced VFI Test Manager
        class VFITestManager:
            """Manage VFI pipeline testing scenarios."""
            
            def __init__(self):
                self.scenario_configs = {
                    "skip": {
                        "description": "Skip RIFE model execution",
                        "kwargs": {"skip_model": True},
                        "expect_error": False,
                        "mock_setup": self._setup_skip_scenario,
                        "validation": self._validate_skip_scenario,
                    },
                    "rife_fail": {
                        "description": "RIFE execution failure",
                        "kwargs": {},
                        "expect_error": True,
                        "mock_setup": self._setup_rife_fail_scenario,
                        "validation": self._validate_error_scenario,
                    },
                    "ffmpeg_fail": {
                        "description": "FFmpeg execution failure",
                        "kwargs": {},
                        "expect_error": True,
                        "mock_setup": self._setup_ffmpeg_fail_scenario,
                        "validation": self._validate_error_scenario,
                    },
                    "sanchez": {
                        "description": "Sanchez false color processing",
                        "kwargs": {"false_colour": True, "res_km": 2},
                        "expect_error": False,
                        "mock_setup": self._setup_sanchez_scenario,
                        "validation": self._validate_sanchez_scenario,
                    },
                    "sanchez_fail": {
                        "description": "Sanchez processing failure",
                        "kwargs": {"false_colour": True, "res_km": 2},
                        "expect_error": False,  # Should handle gracefully
                        "mock_setup": self._setup_sanchez_fail_scenario,
                        "validation": self._validate_sanchez_fail_scenario,
                    },
                }
                
                self.test_scenarios = {
                    "parametrized_scenarios": self._test_parametrized_scenarios,
                    "individual_scenarios": self._test_individual_scenarios,
                    "error_handling": self._test_error_handling,
                    "edge_cases": self._test_edge_cases,
                    "mock_verification": self._test_mock_verification,
                }
            
            def _setup_skip_scenario(self, mock_run, mock_popen, mocker, tmp_path):
                """Setup mocks for skip scenario."""
                raw_output = tmp_path / "out.raw.mp4"
                mock_run.side_effect = None
                mock_popen.side_effect = create_mock_popen(output_file_to_create=raw_output)
                return {"raw_output": raw_output}
            
            def _setup_rife_fail_scenario(self, mock_run, mock_popen, mocker, tmp_path):
                """Setup mocks for RIFE failure scenario."""
                raw_output = tmp_path / "out.raw.mp4"
                mock_run.side_effect = create_mock_subprocess_run(
                    side_effect=subprocess.CalledProcessError(1, "rife")
                )
                mock_popen.side_effect = create_mock_popen(output_file_to_create=raw_output)
                return {"raw_output": raw_output}
            
            def _setup_ffmpeg_fail_scenario(self, mock_run, mock_popen, mocker, tmp_path):
                """Setup mocks for FFmpeg failure scenario."""
                mock_run.side_effect = create_mock_subprocess_run(
                    output_file_to_create=tmp_path / "interp.png"
                )
                mock_popen.side_effect = create_mock_popen(returncode=1, stderr=b"ffmpeg fail")
                return {}
            
            def _setup_sanchez_scenario(self, mock_run, mock_popen, mocker, tmp_path):
                """Setup mocks for Sanchez processing scenario."""
                raw_output = tmp_path / "out.raw.mp4"
                mock_run.side_effect = create_mock_subprocess_run(
                    output_file_to_create=tmp_path / "interp.png"
                )
                mock_popen.side_effect = create_mock_popen(output_file_to_create=raw_output)
                mocker.patch(
                    "goesvfi.pipeline.run_vfi.colourise",
                    create_mock_colourise(output_file_to_create=tmp_path / "fc.png"),
                )
                return {"raw_output": raw_output, "fc_output": tmp_path / "fc.png"}
            
            def _setup_sanchez_fail_scenario(self, mock_run, mock_popen, mocker, tmp_path):
                """Setup mocks for Sanchez failure scenario."""
                raw_output = tmp_path / "out.raw.mp4"
                mock_run.side_effect = create_mock_subprocess_run(
                    output_file_to_create=tmp_path / "interp.png"
                )
                mock_popen.side_effect = create_mock_popen(output_file_to_create=raw_output)
                mocker.patch(
                    "goesvfi.pipeline.run_vfi.colourise",
                    create_mock_colourise(side_effect=RuntimeError("sanchez fail")),
                )
                return {"raw_output": raw_output}
            
            def _validate_skip_scenario(self, results, scenario_data):
                """Validate skip scenario results."""
                assert any(isinstance(r, pathlib.Path) for r in results), "Should return path results"\n                assert scenario_data[\"raw_output\"].exists(), \"Raw output should exist\"\n                return {\"success\": True, \"paths_returned\": True, \"raw_output_exists\": True}\n            \n            def _validate_error_scenario(self, exception_info, scenario_data):\n                \"\"\"Validate error scenario results.\"\"\"\n                from goesvfi.pipeline.exceptions import FFmpegError, ProcessingError, RIFEError\n                \n                assert exception_info is not None, \"Should raise exception\"\n                assert isinstance(exception_info.value, (RuntimeError, ProcessingError, RIFEError, FFmpegError)), \"Should raise expected exception type\"\n                return {\"success\": True, \"exception_raised\": True, \"exception_type\": type(exception_info.value).__name__}\n            \n            def _validate_sanchez_scenario(self, results, scenario_data):\n                \"\"\"Validate Sanchez processing scenario.\"\"\"\n                assert any(isinstance(r, pathlib.Path) for r in results), \"Should return path results\"\n                assert scenario_data[\"raw_output\"].exists(), \"Raw output should exist\"\n                # False color output should be created by the mock\n                return {\"success\": True, \"paths_returned\": True, \"sanchez_processed\": True}\n            \n            def _validate_sanchez_fail_scenario(self, results, scenario_data):\n                \"\"\"Validate Sanchez failure scenario (should handle gracefully).\"\"\"\n                assert any(isinstance(r, pathlib.Path) for r in results), \"Should return path results even if Sanchez fails\"\n                assert scenario_data[\"raw_output\"].exists(), \"Raw output should still exist\"\n                return {\"success\": True, \"paths_returned\": True, \"sanchez_failure_handled\": True}\n            \n            def _test_parametrized_scenarios(self, temp_workspace, mock_registry, mocker):\n                \"\"\"Test all parametrized scenarios together.\"\"\"\n                results = {}\n                \n                for scenario_name, config in self.scenario_configs.items():\n                    # Create scenario-specific workspace\n                    scenario_workspace = self._create_scenario_workspace(temp_workspace, scenario_name)\n                    \n                    # Setup comprehensive mocks for this scenario\n                    with self._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                        # Apply scenario-specific mock setup\n                        scenario_data = config[\"mock_setup\"](\n                            mock_context[\"mock_run\"],\n                            mock_context[\"mock_popen\"],\n                            mocker,\n                            scenario_workspace[\"tmp_path\"]\n                        )\n                        \n                        # Setup capability detector\n                        mock_detector = self._setup_capability_detector(mocker)\n                        \n                        try:\n                            # Execute VFI processing\n                            vfi_results = list(\n                                run_vfi_mod.run_vfi(\n                                    folder=scenario_workspace[\"tmp_path\"],\n                                    output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                    rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                    fps=10,\n                                    num_intermediate_frames=1,\n                                    max_workers=1,\n                                    **config[\"kwargs\"]\n                                )\n                            )\n                            \n                            if config[\"expect_error\"]:\n                                # Should have raised an exception but didn't\n                                results[scenario_name] = {\n                                    \"success\": False,\n                                    \"expected_error\": True,\n                                    \"actual_error\": False,\n                                    \"results\": vfi_results,\n                                }\n                            else:\n                                # Validate successful scenario\n                                validation_result = config[\"validation\"](vfi_results, scenario_data)\n                                results[scenario_name] = {\n                                    \"success\": True,\n                                    \"expected_error\": False,\n                                    \"results\": vfi_results,\n                                    \"validation\": validation_result,\n                                }\n                        \n                        except Exception as e:\n                            if config[\"expect_error\"]:\n                                # Expected error - validate it\n                                exception_info = type('ExceptionInfo', (), {'value': e})()\n                                validation_result = config[\"validation\"](exception_info, scenario_data)\n                                results[scenario_name] = {\n                                    \"success\": True,\n                                    \"expected_error\": True,\n                                    \"actual_error\": True,\n                                    \"exception\": str(e),\n                                    \"validation\": validation_result,\n                                }\n                            else:\n                                # Unexpected error\n                                results[scenario_name] = {\n                                    \"success\": False,\n                                    \"expected_error\": False,\n                                    \"actual_error\": True,\n                                    \"unexpected_exception\": str(e),\n                                }\n                \n                mock_registry[\"parametrized_scenarios\"] = results\n                return results\n            \n            def _test_individual_scenarios(self, temp_workspace, mock_registry, mocker):\n                \"\"\"Test individual scenarios with enhanced validation.\"\"\"\n                individual_results = {}\n                \n                # Test each scenario individually with more detailed verification\n                for scenario_name, config in self.scenario_configs.items():\n                    scenario_workspace = self._create_scenario_workspace(temp_workspace, f\"individual_{scenario_name}\")\n                    \n                    with self._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                        scenario_data = config[\"mock_setup\"](\n                            mock_context[\"mock_run\"],\n                            mock_context[\"mock_popen\"],\n                            mocker,\n                            scenario_workspace[\"tmp_path\"]\n                        )\n                        \n                        mock_detector = self._setup_capability_detector(mocker)\n                        \n                        # Track mock interactions\n                        mock_interactions = {\n                            \"run_called\": False,\n                            \"popen_called\": False,\n                            \"executor_used\": False,\n                            \"image_opened\": False,\n                            \"temp_dir_created\": False,\n                        }\n                        \n                        try:\n                            # Execute and track interactions\n                            vfi_results = list(\n                                run_vfi_mod.run_vfi(\n                                    folder=scenario_workspace[\"tmp_path\"],\n                                    output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                    rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                    fps=10,\n                                    num_intermediate_frames=1,\n                                    max_workers=1,\n                                    **config[\"kwargs\"]\n                                )\n                            )\n                            \n                            # Check mock interactions\n                            mock_interactions[\"run_called\"] = mock_context[\"mock_run\"].called\n                            mock_interactions[\"popen_called\"] = mock_context[\"mock_popen\"].called\n                            mock_interactions[\"executor_used\"] = mock_context[\"mock_exec\"].called\n                            mock_interactions[\"image_opened\"] = mock_context[\"mock_img_open\"].called\n                            mock_interactions[\"temp_dir_created\"] = mock_context[\"mock_tmpdir\"].called\n                            \n                            individual_results[scenario_name] = {\n                                \"success\": not config[\"expect_error\"],\n                                \"results\": vfi_results,\n                                \"mock_interactions\": mock_interactions,\n                                \"scenario_data\": scenario_data,\n                            }\n                        \n                        except Exception as e:\n                            individual_results[scenario_name] = {\n                                \"success\": config[\"expect_error\"],\n                                \"exception\": str(e),\n                                \"exception_type\": type(e).__name__,\n                                \"mock_interactions\": mock_interactions,\n                                \"scenario_data\": scenario_data,\n                            }\n                \n                mock_registry[\"individual_scenarios\"] = individual_results\n                return individual_results\n            \n            def _test_error_handling(self, temp_workspace, mock_registry, mocker):\n                \"\"\"Test comprehensive error handling scenarios.\"\"\"\n                error_tests = {}\n                \n                # Test different types of errors\n                error_scenarios = [\n                    {\n                        \"name\": \"rife_subprocess_error\",\n                        \"setup\": lambda mr, mp: mr.side_effect = create_mock_subprocess_run(\n                            side_effect=subprocess.CalledProcessError(1, \"rife\", stderr=\"RIFE error\")\n                        ),\n                        \"expected_exception\": \"RIFEError\",\n                    },\n                    {\n                        \"name\": \"ffmpeg_popen_error\",\n                        \"setup\": lambda mr, mp: (\n                            setattr(mr, 'side_effect', create_mock_subprocess_run(output_file_to_create=temp_workspace[\"tmp_path\"] / \"interp.png\")),\n                            setattr(mp, 'side_effect', create_mock_popen(returncode=2, stderr=b\"FFmpeg critical error\"))\n                        ),\n                        \"expected_exception\": \"FFmpegError\",\n                    },\n                    {\n                        \"name\": \"general_runtime_error\",\n                        \"setup\": lambda mr, mp: mr.side_effect = RuntimeError(\"General processing error\"),\n                        \"expected_exception\": \"RuntimeError\",\n                    },\n                ]\n                \n                for error_scenario in error_scenarios:\n                    scenario_workspace = self._create_scenario_workspace(temp_workspace, error_scenario[\"name\"])\n                    \n                    with self._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                        # Apply error-specific setup\n                        error_scenario[\"setup\"](mock_context[\"mock_run\"], mock_context[\"mock_popen\"])\n                        \n                        mock_detector = self._setup_capability_detector(mocker)\n                        \n                        try:\n                            list(\n                                run_vfi_mod.run_vfi(\n                                    folder=scenario_workspace[\"tmp_path\"],\n                                    output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                    rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                    fps=10,\n                                    num_intermediate_frames=1,\n                                    max_workers=1,\n                                )\n                            )\n                            \n                            error_tests[error_scenario[\"name\"]] = {\n                                \"success\": False,\n                                \"expected_error\": True,\n                                \"actual_error\": False,\n                            }\n                        \n                        except Exception as e:\n                            error_tests[error_scenario[\"name\"]] = {\n                                \"success\": True,\n                                \"expected_error\": True,\n                                \"actual_error\": True,\n                                \"exception_type\": type(e).__name__,\n                                \"exception_message\": str(e),\n                                \"matches_expected\": error_scenario[\"expected_exception\"] in type(e).__name__,\n                            }\n                \n                mock_registry[\"error_handling\"] = error_tests\n                return error_tests\n            \n            def _test_edge_cases(self, temp_workspace, mock_registry, mocker):\n                \"\"\"Test edge cases and boundary conditions.\"\"\"\n                edge_case_tests = {}\n                \n                # Test different parameter combinations\n                edge_cases = [\n                    {\n                        \"name\": \"high_worker_count\",\n                        \"params\": {\"max_workers\": 8, \"num_intermediate_frames\": 3},\n                    },\n                    {\n                        \"name\": \"single_frame\",\n                        \"params\": {\"num_intermediate_frames\": 0},\n                    },\n                    {\n                        \"name\": \"high_fps\",\n                        \"params\": {\"fps\": 60, \"num_intermediate_frames\": 2},\n                    },\n                    {\n                        \"name\": \"minimal_setup\",\n                        \"params\": {\"fps\": 1, \"num_intermediate_frames\": 1, \"max_workers\": 1},\n                    },\n                ]\n                \n                for edge_case in edge_cases:\n                    scenario_workspace = self._create_scenario_workspace(temp_workspace, edge_case[\"name\"])\n                    \n                    with self._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                        # Setup successful scenario\n                        raw_output = scenario_workspace[\"tmp_path\"] / \"out.raw.mp4\"\n                        mock_context[\"mock_run\"].side_effect = create_mock_subprocess_run(\n                            output_file_to_create=scenario_workspace[\"tmp_path\"] / \"interp.png\"\n                        )\n                        mock_context[\"mock_popen\"].side_effect = create_mock_popen(output_file_to_create=raw_output)\n                        \n                        mock_detector = self._setup_capability_detector(mocker)\n                        \n                        try:\n                            vfi_results = list(\n                                run_vfi_mod.run_vfi(\n                                    folder=scenario_workspace[\"tmp_path\"],\n                                    output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                    rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                    **edge_case[\"params\"]\n                                )\n                            )\n                            \n                            edge_case_tests[edge_case[\"name\"]] = {\n                                \"success\": True,\n                                \"params\": edge_case[\"params\"],\n                                \"results_count\": len(vfi_results),\n                                \"raw_output_exists\": raw_output.exists(),\n                            }\n                        \n                        except Exception as e:\n                            edge_case_tests[edge_case[\"name\"]] = {\n                                \"success\": False,\n                                \"params\": edge_case[\"params\"],\n                                \"exception\": str(e),\n                                \"exception_type\": type(e).__name__,\n                            }\n                \n                mock_registry[\"edge_cases\"] = edge_case_tests\n                return edge_case_tests\n            \n            def _test_mock_verification(self, temp_workspace, mock_registry, mocker):\n                \"\"\"Test mock verification and interaction patterns.\"\"\"\n                mock_verification_tests = {}\n                \n                # Test different mock interaction patterns\n                verification_scenarios = [\n                    {\n                        \"name\": \"full_pipeline\",\n                        \"setup\": \"complete_success\",\n                        \"verify_interactions\": [\"run\", \"popen\", \"executor\", \"image_open\", \"temp_dir\"],\n                    },\n                    {\n                        \"name\": \"skip_model_pipeline\",\n                        \"setup\": \"skip_model\",\n                        \"verify_interactions\": [\"popen\", \"image_open\", \"temp_dir\"],\n                        \"skip_interactions\": [\"run\"],\n                    },\n                    {\n                        \"name\": \"error_cleanup\",\n                        \"setup\": \"rife_error\",\n                        \"verify_interactions\": [\"run\", \"temp_dir\"],\n                        \"expect_error\": True,\n                    },\n                ]\n                \n                for verification_scenario in verification_scenarios:\n                    scenario_workspace = self._create_scenario_workspace(temp_workspace, verification_scenario[\"name\"])\n                    \n                    with self._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                        # Setup based on scenario\n                        if verification_scenario[\"setup\"] == \"complete_success\":\n                            raw_output = scenario_workspace[\"tmp_path\"] / \"out.raw.mp4\"\n                            mock_context[\"mock_run\"].side_effect = create_mock_subprocess_run(\n                                output_file_to_create=scenario_workspace[\"tmp_path\"] / \"interp.png\"\n                            )\n                            mock_context[\"mock_popen\"].side_effect = create_mock_popen(output_file_to_create=raw_output)\n                            kwargs = {}\n                        elif verification_scenario[\"setup\"] == \"skip_model\":\n                            raw_output = scenario_workspace[\"tmp_path\"] / \"out.raw.mp4\"\n                            mock_context[\"mock_run\"].side_effect = None\n                            mock_context[\"mock_popen\"].side_effect = create_mock_popen(output_file_to_create=raw_output)\n                            kwargs = {\"skip_model\": True}\n                        elif verification_scenario[\"setup\"] == \"rife_error\":\n                            mock_context[\"mock_run\"].side_effect = create_mock_subprocess_run(\n                                side_effect=subprocess.CalledProcessError(1, \"rife\")\n                            )\n                            kwargs = {}\n                        \n                        mock_detector = self._setup_capability_detector(mocker)\n                        \n                        try:\n                            vfi_results = list(\n                                run_vfi_mod.run_vfi(\n                                    folder=scenario_workspace[\"tmp_path\"],\n                                    output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                    rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                    fps=10,\n                                    num_intermediate_frames=1,\n                                    max_workers=1,\n                                    **kwargs\n                                )\n                            )\n                            \n                            # Verify expected interactions\n                            interactions = {}\n                            if \"run\" in verification_scenario[\"verify_interactions\"]:\n                                interactions[\"run_called\"] = mock_context[\"mock_run\"].called\n                            if \"popen\" in verification_scenario[\"verify_interactions\"]:\n                                interactions[\"popen_called\"] = mock_context[\"mock_popen\"].called\n                            if \"executor\" in verification_scenario[\"verify_interactions\"]:\n                                interactions[\"executor_called\"] = mock_context[\"mock_exec\"].called\n                            if \"image_open\" in verification_scenario[\"verify_interactions\"]:\n                                interactions[\"image_open_called\"] = mock_context[\"mock_img_open\"].called\n                            if \"temp_dir\" in verification_scenario[\"verify_interactions\"]:\n                                interactions[\"temp_dir_called\"] = mock_context[\"mock_tmpdir\"].called\n                            \n                            # Verify skipped interactions\n                            if \"skip_interactions\" in verification_scenario:\n                                for skip_interaction in verification_scenario[\"skip_interactions\"]:\n                                    if skip_interaction == \"run\":\n                                        interactions[\"run_not_called\"] = not mock_context[\"mock_run\"].called\n                            \n                            mock_verification_tests[verification_scenario[\"name\"]] = {\n                                \"success\": not verification_scenario.get(\"expect_error\", False),\n                                \"interactions\": interactions,\n                                \"results_count\": len(vfi_results),\n                            }\n                        \n                        except Exception as e:\n                            mock_verification_tests[verification_scenario[\"name\"]] = {\n                                \"success\": verification_scenario.get(\"expect_error\", False),\n                                \"exception\": str(e),\n                                \"exception_type\": type(e).__name__,\n                            }\n                \n                mock_registry[\"mock_verification\"] = mock_verification_tests\n                return mock_verification_tests\n            \n            def _create_scenario_workspace(self, temp_workspace, scenario_name):\n                \"\"\"Create workspace for specific scenario.\"\"\"\n                scenario_dir = temp_workspace[\"tmp_path\"] / scenario_name\n                scenario_dir.mkdir(exist_ok=True)\n                \n                # Create dummy images\n                img_paths = self._make_dummy_images(scenario_dir, 2)\n                \n                return {\n                    \"tmp_path\": scenario_dir,\n                    \"img_paths\": img_paths,\n                    \"output_mp4\": scenario_dir / \"out.mp4\",\n                    \"rife_exe\": scenario_dir / \"rife\",\n                }\n            \n            def _setup_comprehensive_mocks(self, scenario_workspace, mocker):\n                \"\"\"Setup comprehensive mocks for VFI testing.\"\"\"\n                return patch.multiple(\n                    \"goesvfi.pipeline.run_vfi\",\n                    **{\n                        \"subprocess.run\": patch(\"goesvfi.pipeline.run_vfi.subprocess.run\"),\n                        \"subprocess.Popen\": patch(\"goesvfi.pipeline.run_vfi.subprocess.Popen\"),\n                        \"ProcessPoolExecutor\": patch(\"goesvfi.pipeline.run_vfi.ProcessPoolExecutor\"),\n                        \"Image.open\": patch(\"goesvfi.pipeline.run_vfi.Image.open\"),\n                        \"tempfile.TemporaryDirectory\": patch(\"goesvfi.pipeline.run_vfi.tempfile.TemporaryDirectory\"),\n                    }\n                ).start(), self._setup_additional_mocks(scenario_workspace, mocker)\n            \n            def _setup_additional_mocks(self, scenario_workspace, mocker):\n                \"\"\"Setup additional mocks and patches.\"\"\"\n                # Patch pathlib methods\n                mocker.patch.object(pathlib.Path, \"glob\", return_value=scenario_workspace[\"img_paths\"])\n                mocker.patch(\"goesvfi.pipeline.run_vfi.pathlib.Path.exists\", return_value=True)\n                mocker.patch(\"goesvfi.pipeline.run_vfi.pathlib.Path.unlink\", lambda *_a, **_k: None)\n                \n                # Setup executor mock\n                mock_exec = mocker.patch(\"goesvfi.pipeline.run_vfi.ProcessPoolExecutor\")\n                mock_exec.return_value.__enter__.return_value = MagicMock(map=lambda fn, it: it)\n                \n                # Setup image mock\n                mock_img = MagicMock()\n                mock_img.size = (4, 4)\n                mock_img.__enter__.return_value = mock_img\n                mock_img_open = mocker.patch(\"goesvfi.pipeline.run_vfi.Image.open\", return_value=mock_img)\n                \n                # Setup temp directory mock\n                mock_tmpdir = mocker.patch(\"goesvfi.pipeline.run_vfi.tempfile.TemporaryDirectory\")\n                mock_tmpdir.return_value.__enter__.return_value = scenario_workspace[\"tmp_path\"]\n                \n                return {\n                    \"mock_exec\": mock_exec,\n                    \"mock_img_open\": mock_img_open,\n                    \"mock_tmpdir\": mock_tmpdir,\n                }\n            \n            def _setup_capability_detector(self, mocker):\n                \"\"\"Setup RIFE capability detector mock.\"\"\"\n                from goesvfi.utils.rife_analyzer import RifeCapabilityDetector\n                \n                mock_detector = mocker.MagicMock(spec=RifeCapabilityDetector)\n                mock_detector.supports_thread_spec.return_value = True\n                return mocker.patch(\"goesvfi.pipeline.run_vfi.RifeCapabilityDetector\", return_value=mock_detector)\n            \n            def _make_dummy_images(self, tmp_path, count):\n                \"\"\"Create dummy PNG images for testing.\"\"\"\n                import numpy as np\n                from PIL import Image\n                \n                paths = []\n                for i in range(count):\n                    img_path = tmp_path / f\"img{i}.png\"\n                    img_array = np.zeros((4, 4, 3), dtype=np.uint8)\n                    img = Image.fromarray(img_array)\n                    img_path.parent.mkdir(parents=True, exist_ok=True)\n                    img.save(img_path, format=\"PNG\")\n                    paths.append(img_path)\n                return paths\n            \n            def run_test_scenario(self, scenario: str, temp_workspace: Dict[str, Any], mock_registry: Dict[str, Any], mocker):\n                \"\"\"Run specified test scenario.\"\"\"\n                return self.test_scenarios[scenario](temp_workspace, mock_registry, mocker)\n        \n        # Enhanced Result Analyzer\n        class ResultAnalyzer:\n            \"\"\"Analyze VFI test results for correctness and completeness.\"\"\"\n            \n            def __init__(self):\n                self.analysis_rules = {\n                    \"scenario_coverage\": self._analyze_scenario_coverage,\n                    \"error_handling\": self._analyze_error_handling,\n                    \"mock_interactions\": self._analyze_mock_interactions,\n                    \"performance_metrics\": self._analyze_performance_metrics,\n                }\n            \n            def _analyze_scenario_coverage(self, results: Dict[str, Any]) -> Dict[str, Any]:\n                \"\"\"Analyze scenario coverage.\"\"\"\n                return {\n                    \"total_scenarios\": len(results),\n                    \"successful_scenarios\": sum(1 for r in results.values() if r.get(\"success\")),\n                    \"error_scenarios\": sum(1 for r in results.values() if r.get(\"expected_error\")),\n                    \"coverage_rate\": sum(1 for r in results.values() if r.get(\"success\")) / len(results) if results else 0,\n                }\n            \n            def _analyze_error_handling(self, results: Dict[str, Any]) -> Dict[str, Any]:\n                \"\"\"Analyze error handling effectiveness.\"\"\"\n                return {\n                    \"errors_caught\": sum(1 for r in results.values() if r.get(\"actual_error\")),\n                    \"expected_errors\": sum(1 for r in results.values() if r.get(\"expected_error\")),\n                    \"unexpected_errors\": sum(1 for r in results.values() if r.get(\"unexpected_exception\")),\n                    \"error_handling_rate\": self._calculate_error_handling_rate(results),\n                }\n            \n            def _analyze_mock_interactions(self, results: Dict[str, Any]) -> Dict[str, Any]:\n                \"\"\"Analyze mock interaction patterns.\"\"\"\n                return {\n                    \"total_interactions\": self._count_total_interactions(results),\n                    \"successful_interactions\": self._count_successful_interactions(results),\n                    \"interaction_coverage\": self._calculate_interaction_coverage(results),\n                }\n            \n            def _analyze_performance_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:\n                \"\"\"Analyze performance-related metrics.\"\"\"\n                return {\n                    \"total_tests\": len(results),\n                    \"successful_tests\": sum(1 for r in results.values() if r.get(\"success\")),\n                    \"average_results_per_test\": self._calculate_average_results(results),\n                    \"resource_utilization\": self._analyze_resource_utilization(results),\n                }\n            \n            def _calculate_error_handling_rate(self, results: Dict[str, Any]) -> float:\n                \"\"\"Calculate error handling rate.\"\"\"\n                error_tests = [r for r in results.values() if r.get(\"expected_error\")]\n                handled_correctly = [r for r in error_tests if r.get(\"actual_error\") and r.get(\"success\")]\n                return len(handled_correctly) / len(error_tests) if error_tests else 1.0\n            \n            def _count_total_interactions(self, results: Dict[str, Any]) -> int:\n                \"\"\"Count total mock interactions.\"\"\"\n                total = 0\n                for result in results.values():\n                    if \"mock_interactions\" in result:\n                        total += sum(1 for v in result[\"mock_interactions\"].values() if v)\n                return total\n            \n            def _count_successful_interactions(self, results: Dict[str, Any]) -> int:\n                \"\"\"Count successful mock interactions.\"\"\"\n                successful = 0\n                for result in results.values():\n                    if result.get(\"success\") and \"mock_interactions\" in result:\n                        successful += sum(1 for v in result[\"mock_interactions\"].values() if v)\n                return successful\n            \n            def _calculate_interaction_coverage(self, results: Dict[str, Any]) -> float:\n                \"\"\"Calculate interaction coverage rate.\"\"\"\n                total = self._count_total_interactions(results)\n                successful = self._count_successful_interactions(results)\n                return successful / total if total > 0 else 0.0\n            \n            def _calculate_average_results(self, results: Dict[str, Any]) -> float:\n                \"\"\"Calculate average results per test.\"\"\"\n                result_counts = []\n                for result in results.values():\n                    if \"results_count\" in result:\n                        result_counts.append(result[\"results_count\"])\n                    elif \"results\" in result and isinstance(result[\"results\"], list):\n                        result_counts.append(len(result[\"results\"]))\n                \n                return sum(result_counts) / len(result_counts) if result_counts else 0.0\n            \n            def _analyze_resource_utilization(self, results: Dict[str, Any]) -> Dict[str, Any]:\n                \"\"\"Analyze resource utilization patterns.\"\"\"\n                return {\n                    \"executor_usage\": sum(1 for r in results.values() if r.get(\"mock_interactions\", {}).get(\"executor_used\")),\n                    \"temp_dir_usage\": sum(1 for r in results.values() if r.get(\"mock_interactions\", {}).get(\"temp_dir_created\")),\n                    \"image_processing\": sum(1 for r in results.values() if r.get(\"mock_interactions\", {}).get(\"image_opened\")),\n                }\n            \n            def analyze_results(self, results: Dict[str, Any], analysis_types: List[str] = None) -> Dict[str, Any]:\n                \"\"\"Analyze results using specified analysis types.\"\"\"\n                if analysis_types is None:\n                    analysis_types = list(self.analysis_rules.keys())\n                \n                analysis_results = {}\n                for analysis_type in analysis_types:\n                    if analysis_type in self.analysis_rules:\n                        analysis_results[analysis_type] = self.analysis_rules[analysis_type](results)\n                \n                return analysis_results\n        \n        return {\n            \"test_manager\": VFITestManager(),\n            \"analyzer\": ResultAnalyzer(),\n        }\n\n    @pytest.fixture()\n    def temp_workspace(self, tmp_path):\n        \"\"\"Create temporary workspace for VFI testing.\"\"\"\n        workspace = {\n            \"tmp_path\": tmp_path,\n        }\n        return workspace\n\n    @pytest.fixture()\n    def mock_registry(self):\n        \"\"\"Registry for storing mock interaction results.\"\"\"\n        return {}\n\n    def test_vfi_comprehensive_parametrized_scenarios(self, vfi_test_components, temp_workspace, mock_registry, mocker) -> None:\n        \"\"\"Test comprehensive VFI parametrized scenarios with all functionality.\"\"\"\n        components = vfi_test_components\n        test_manager = components[\"test_manager\"]\n        analyzer = components[\"analyzer\"]\n        \n        # Define comprehensive VFI test scenarios\n        vfi_scenarios = [\n            {\n                \"name\": \"Parametrized Scenarios\",\n                \"test_type\": \"parametrized_scenarios\",\n                \"analysis_types\": [\"scenario_coverage\", \"error_handling\"],\n                \"expected_scenarios\": 5,  # skip, rife_fail, ffmpeg_fail, sanchez, sanchez_fail\n            },\n            {\n                \"name\": \"Individual Scenarios\",\n                \"test_type\": \"individual_scenarios\",\n                \"analysis_types\": [\"mock_interactions\", \"performance_metrics\"],\n                \"expected_scenarios\": 5,\n            },\n            {\n                \"name\": \"Error Handling\",\n                \"test_type\": \"error_handling\",\n                \"analysis_types\": [\"error_handling\"],\n                \"expected_errors\": 3,  # rife_subprocess_error, ffmpeg_popen_error, general_runtime_error\n            },\n            {\n                \"name\": \"Edge Cases\",\n                \"test_type\": \"edge_cases\",\n                \"analysis_types\": [\"performance_metrics\"],\n                \"expected_cases\": 4,  # high_worker_count, single_frame, high_fps, minimal_setup\n            },\n            {\n                \"name\": \"Mock Verification\",\n                \"test_type\": \"mock_verification\",\n                \"analysis_types\": [\"mock_interactions\"],\n                \"expected_verifications\": 3,  # full_pipeline, skip_model_pipeline, error_cleanup\n            },\n        ]\n        \n        # Test each VFI scenario\n        all_results = {}\n        \n        for scenario in vfi_scenarios:\n            try:\n                # Run VFI test scenario\n                scenario_results = test_manager.run_test_scenario(\n                    scenario[\"test_type\"], temp_workspace, mock_registry, mocker\n                )\n                \n                # Analyze results\n                if scenario[\"analysis_types\"]:\n                    analysis_results = analyzer.analyze_results(\n                        scenario_results, scenario[\"analysis_types\"]\n                    )\n                    scenario_results[\"analysis\"] = analysis_results\n                \n                # Verify scenario-specific expectations\n                if scenario[\"name\"] == \"Parametrized Scenarios\":\n                    # Should test all 5 original parametrized scenarios\n                    assert len(scenario_results) >= scenario[\"expected_scenarios\"], f\"Should test {scenario['expected_scenarios']} parametrized scenarios\"\n                    \n                    # Check specific scenarios\n                    expected_scenario_names = [\"skip\", \"rife_fail\", \"ffmpeg_fail\", \"sanchez\", \"sanchez_fail\"]\n                    for scenario_name in expected_scenario_names:\n                        assert scenario_name in scenario_results, f\"Should include {scenario_name} scenario\"\n                        \n                        scenario_result = scenario_results[scenario_name]\n                        if scenario_name in [\"rife_fail\", \"ffmpeg_fail\"]:\n                            # Error scenarios\n                            assert scenario_result.get(\"expected_error\"), f\"{scenario_name} should expect error\"\n                            assert scenario_result.get(\"actual_error\") or scenario_result.get(\"success\"), f\"{scenario_name} should handle error correctly\"\n                        else:\n                            # Success scenarios\n                            assert scenario_result.get(\"success\"), f\"{scenario_name} should succeed\"\n                    \n                    # Check analysis\n                    if \"analysis\" in scenario_results:\n                        coverage_analysis = scenario_results[\"analysis\"][\"scenario_coverage\"]\n                        assert coverage_analysis[\"total_scenarios\"] >= scenario[\"expected_scenarios\"], \"Should cover all scenarios\"\n                \n                elif scenario[\"name\"] == \"Individual Scenarios\":\n                    # Should test individual scenarios with detailed verification\n                    assert len(scenario_results) >= scenario[\"expected_scenarios\"], f\"Should test {scenario['expected_scenarios']} individual scenarios\"\n                    \n                    # All individual scenarios should have mock interaction data\n                    for scenario_name, scenario_result in scenario_results.items():\n                        if scenario_name != \"analysis\":\n                            assert \"mock_interactions\" in scenario_result or \"exception\" in scenario_result, f\"Should have interaction data for {scenario_name}\"\n                \n                elif scenario[\"name\"] == \"Error Handling\":\n                    # Should test different error types\n                    assert len(scenario_results) >= scenario[\"expected_errors\"], f\"Should test {scenario['expected_errors']} error types\"\n                    \n                    # Check specific error types\n                    error_types = [\"rife_subprocess_error\", \"ffmpeg_popen_error\", \"general_runtime_error\"]\n                    for error_type in error_types:\n                        assert error_type in scenario_results, f\"Should test {error_type}\"\n                        error_result = scenario_results[error_type]\n                        assert error_result.get(\"actual_error\"), f\"{error_type} should raise an error\"\n                \n                elif scenario[\"name\"] == \"Edge Cases\":\n                    # Should test various edge cases\n                    assert len(scenario_results) >= scenario[\"expected_cases\"], f\"Should test {scenario['expected_cases']} edge cases\"\n                    \n                    # Check specific edge cases\n                    edge_case_names = [\"high_worker_count\", \"single_frame\", \"high_fps\", \"minimal_setup\"]\n                    for edge_case in edge_case_names:\n                        assert edge_case in scenario_results, f\"Should test {edge_case} edge case\"\n                \n                elif scenario[\"name\"] == \"Mock Verification\":\n                    # Should test mock interaction patterns\n                    assert len(scenario_results) >= scenario[\"expected_verifications\"], f\"Should test {scenario['expected_verifications']} verification patterns\"\n                    \n                    # Check specific verification scenarios\n                    verification_names = [\"full_pipeline\", \"skip_model_pipeline\", \"error_cleanup\"]\n                    for verification in verification_names:\n                        assert verification in scenario_results, f\"Should test {verification} verification\"\n                \n                all_results[scenario[\"name\"]] = scenario_results\n                \n            except Exception as e:\n                pytest.fail(f\"Unexpected error in {scenario['name']}: {e}\")\n        \n        # Overall validation\n        assert len(all_results) == len(vfi_scenarios), \"Not all VFI scenarios completed\"\n\n    def test_vfi_original_parametrized_compatibility(self, vfi_test_components, temp_workspace, mocker) -> None:\n        \"\"\"Test compatibility with original parametrized test structure.\"\"\"\n        components = vfi_test_components\n        test_manager = components[\"test_manager\"]\n        \n        # Test the exact original parametrized scenarios\n        original_scenarios = [\n            (\"skip\", False),\n            (\"rife_fail\", True),\n            (\"ffmpeg_fail\", True),\n            (\"sanchez\", False),\n            (\"sanchez_fail\", False),\n        ]\n        \n        # Test each original scenario individually\n        for scenario_name, expect_error in original_scenarios:\n            scenario_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"original_{scenario_name}\")\n            config = test_manager.scenario_configs[scenario_name]\n            \n            with test_manager._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                # Apply scenario-specific setup\n                scenario_data = config[\"mock_setup\"](\n                    mock_context[0][\"subprocess.run\"],  # mock_run from patch.multiple\n                    mock_context[0][\"subprocess.Popen\"],  # mock_popen from patch.multiple\n                    mocker,\n                    scenario_workspace[\"tmp_path\"]\n                )\n                \n                # Setup capability detector\n                mock_detector = test_manager._setup_capability_detector(mocker)\n                \n                # Execute VFI processing\n                if expect_error:\n                    from goesvfi.pipeline.exceptions import FFmpegError, ProcessingError, RIFEError\n                    \n                    with pytest.raises((RuntimeError, ProcessingError, RIFEError, FFmpegError)):\n                        list(\n                            run_vfi_mod.run_vfi(\n                                folder=scenario_workspace[\"tmp_path\"],\n                                output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                fps=10,\n                                num_intermediate_frames=1,\n                                max_workers=1,\n                                **config[\"kwargs\"]\n                            )\n                        )\n                else:\n                    results = list(\n                        run_vfi_mod.run_vfi(\n                            folder=scenario_workspace[\"tmp_path\"],\n                            output_mp4_path=scenario_workspace[\"output_mp4\"],\n                            rife_exe_path=scenario_workspace[\"rife_exe\"],\n                            fps=10,\n                            num_intermediate_frames=1,\n                            max_workers=1,\n                            **config[\"kwargs\"]\n                        )\n                    )\n                    \n                    # Verify results match original expectations\n                    assert any(isinstance(r, pathlib.Path) for r in results), f\"Should return path results for {scenario_name}\"\n                    \n                    if \"raw_output\" in scenario_data:\n                        assert scenario_data[\"raw_output\"].exists(), f\"Raw output should exist for {scenario_name}\"\n\n    def test_vfi_performance_and_stress_scenarios(self, vfi_test_components, temp_workspace, mocker) -> None:\n        \"\"\"Test VFI performance characteristics and stress scenarios.\"\"\"\n        components = vfi_test_components\n        test_manager = components[\"test_manager\"]\n        \n        # Performance and stress test scenarios\n        performance_scenarios = [\n            {\n                \"name\": \"Rapid Scenario Switching\",\n                \"test\": lambda: self._test_rapid_scenario_switching(temp_workspace, test_manager, mocker),\n            },\n            {\n                \"name\": \"High Worker Count Stress\",\n                \"test\": lambda: self._test_high_worker_count_stress(temp_workspace, test_manager, mocker),\n            },\n            {\n                \"name\": \"Multiple Image Processing\",\n                \"test\": lambda: self._test_multiple_image_processing(temp_workspace, test_manager, mocker),\n            },\n            {\n                \"name\": \"Error Recovery Patterns\",\n                \"test\": lambda: self._test_error_recovery_patterns(temp_workspace, test_manager, mocker),\n            },\n        ]\n        \n        # Test each performance scenario\n        for scenario in performance_scenarios:\n            try:\n                result = scenario[\"test\"]()\n                assert result is not None, f\"Performance test {scenario['name']} returned None\"\n                assert result.get(\"success\", False), f\"Performance test {scenario['name']} failed\"\n            except Exception as e:\n                # Some performance tests may have expected limitations\n                assert \"expected\" in str(e).lower() or \"limitation\" in str(e).lower(), (\n                    f\"Unexpected error in performance test {scenario['name']}: {e}\"\n                )\n\n    def _test_rapid_scenario_switching(self, temp_workspace, test_manager, mocker):\n        \"\"\"Test rapid switching between scenarios.\"\"\"\n        successful_switches = 0\n        total_switches = 10\n        \n        scenarios = list(test_manager.scenario_configs.keys())\n        \n        for i in range(total_switches):\n            scenario_name = scenarios[i % len(scenarios)]\n            config = test_manager.scenario_configs[scenario_name]\n            \n            scenario_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"rapid_{i}_{scenario_name}\")\n            \n            try:\n                with test_manager._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                    scenario_data = config[\"mock_setup\"](\n                        mock_context[0][\"subprocess.run\"],\n                        mock_context[0][\"subprocess.Popen\"],\n                        mocker,\n                        scenario_workspace[\"tmp_path\"]\n                    )\n                    \n                    mock_detector = test_manager._setup_capability_detector(mocker)\n                    \n                    try:\n                        results = list(\n                            run_vfi_mod.run_vfi(\n                                folder=scenario_workspace[\"tmp_path\"],\n                                output_mp4_path=scenario_workspace[\"output_mp4\"],\n                                rife_exe_path=scenario_workspace[\"rife_exe\"],\n                                fps=10,\n                                num_intermediate_frames=1,\n                                max_workers=1,\n                                **config[\"kwargs\"]\n                            )\n                        )\n                        \n                        if not config[\"expect_error\"]:\n                            successful_switches += 1\n                        else:\n                            # Error scenarios that don't raise exceptions count as failures\n                            pass\n                    \n                    except Exception:\n                        if config[\"expect_error\"]:\n                            successful_switches += 1  # Expected error\n                        # Unexpected errors are not counted\n            \n            except Exception:\n                # Setup or other errors\n                pass\n        \n        return {\n            \"success\": True,\n            \"successful_switches\": successful_switches,\n            \"total_switches\": total_switches,\n            \"success_rate\": successful_switches / total_switches,\n        }\n\n    def _test_high_worker_count_stress(self, temp_workspace, test_manager, mocker):\n        \"\"\"Test stress with high worker counts.\"\"\"\n        worker_counts = [1, 2, 4, 8, 16]\n        successful_tests = 0\n        \n        for worker_count in worker_counts:\n            scenario_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"workers_{worker_count}\")\n            \n            try:\n                with test_manager._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                    # Setup successful scenario\n                    raw_output = scenario_workspace[\"tmp_path\"] / \"out.raw.mp4\"\n                    mock_context[0][\"subprocess.run\"].side_effect = create_mock_subprocess_run(\n                        output_file_to_create=scenario_workspace[\"tmp_path\"] / \"interp.png\"\n                    )\n                    mock_context[0][\"subprocess.Popen\"].side_effect = create_mock_popen(output_file_to_create=raw_output)\n                    \n                    mock_detector = test_manager._setup_capability_detector(mocker)\n                    \n                    results = list(\n                        run_vfi_mod.run_vfi(\n                            folder=scenario_workspace[\"tmp_path\"],\n                            output_mp4_path=scenario_workspace[\"output_mp4\"],\n                            rife_exe_path=scenario_workspace[\"rife_exe\"],\n                            fps=10,\n                            num_intermediate_frames=1,\n                            max_workers=worker_count,\n                        )\n                    )\n                    \n                    successful_tests += 1\n            \n            except Exception:\n                # High worker counts might fail\n                pass\n        \n        return {\n            \"success\": True,\n            \"successful_tests\": successful_tests,\n            \"total_worker_tests\": len(worker_counts),\n            \"max_workers_tested\": max(worker_counts),\n        }\n\n    def _test_multiple_image_processing(self, temp_workspace, test_manager, mocker):\n        \"\"\"Test processing multiple images.\"\"\"\n        image_counts = [2, 5, 10, 20]\n        successful_tests = 0\n        \n        for image_count in image_counts:\n            scenario_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"images_{image_count}\")\n            \n            # Create additional images\n            additional_images = test_manager._make_dummy_images(scenario_workspace[\"tmp_path\"], image_count)\n            scenario_workspace[\"img_paths\"] = additional_images\n            \n            try:\n                with test_manager._setup_comprehensive_mocks(scenario_workspace, mocker) as mock_context:\n                    # Update glob mock to return more images\n                    mocker.patch.object(pathlib.Path, \"glob\", return_value=additional_images)\n                    \n                    # Setup successful scenario\n                    raw_output = scenario_workspace[\"tmp_path\"] / \"out.raw.mp4\"\n                    mock_context[0][\"subprocess.run\"].side_effect = create_mock_subprocess_run(\n                        output_file_to_create=scenario_workspace[\"tmp_path\"] / \"interp.png\"\n                    )\n                    mock_context[0][\"subprocess.Popen\"].side_effect = create_mock_popen(output_file_to_create=raw_output)\n                    \n                    mock_detector = test_manager._setup_capability_detector(mocker)\n                    \n                    results = list(\n                        run_vfi_mod.run_vfi(\n                            folder=scenario_workspace[\"tmp_path\"],\n                            output_mp4_path=scenario_workspace[\"output_mp4\"],\n                            rife_exe_path=scenario_workspace[\"rife_exe\"],\n                            fps=10,\n                            num_intermediate_frames=1,\n                            max_workers=1,\n                        )\n                    )\n                    \n                    successful_tests += 1\n            \n            except Exception:\n                # Large image counts might fail\n                pass\n        \n        return {\n            \"success\": True,\n            \"successful_tests\": successful_tests,\n            \"total_image_tests\": len(image_counts),\n            \"max_images_tested\": max(image_counts),\n        }\n\n    def _test_error_recovery_patterns(self, temp_workspace, test_manager, mocker):\n        \"\"\"Test error recovery patterns.\"\"\"\n        recovery_tests = 0\n        successful_recoveries = 0\n        \n        # Test error followed by success\n        error_success_pairs = [\n            (\"rife_fail\", \"skip\"),\n            (\"ffmpeg_fail\", \"sanchez\"),\n            (\"sanchez_fail\", \"skip\"),\n        ]\n        \n        for error_scenario, success_scenario in error_success_pairs:\n            recovery_tests += 1\n            \n            # Test error scenario first\n            error_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"error_{error_scenario}\")\n            error_config = test_manager.scenario_configs[error_scenario]\n            \n            try:\n                with test_manager._setup_comprehensive_mocks(error_workspace, mocker) as mock_context:\n                    error_config[\"mock_setup\"](\n                        mock_context[0][\"subprocess.run\"],\n                        mock_context[0][\"subprocess.Popen\"],\n                        mocker,\n                        error_workspace[\"tmp_path\"]\n                    )\n                    \n                    mock_detector = test_manager._setup_capability_detector(mocker)\n                    \n                    try:\n                        list(\n                            run_vfi_mod.run_vfi(\n                                folder=error_workspace[\"tmp_path\"],\n                                output_mp4_path=error_workspace[\"output_mp4\"],\n                                rife_exe_path=error_workspace[\"rife_exe\"],\n                                fps=10,\n                                num_intermediate_frames=1,\n                                max_workers=1,\n                                **error_config[\"kwargs\"]\n                            )\n                        )\n                    except Exception:\n                        # Error expected, now test recovery\n                        pass\n                \n                # Test success scenario after error\n                success_workspace = test_manager._create_scenario_workspace(temp_workspace, f\"success_{success_scenario}\")\n                success_config = test_manager.scenario_configs[success_scenario]\n                \n                with test_manager._setup_comprehensive_mocks(success_workspace, mocker) as mock_context:\n                    success_config[\"mock_setup\"](\n                        mock_context[0][\"subprocess.run\"],\n                        mock_context[0][\"subprocess.Popen\"],\n                        mocker,\n                        success_workspace[\"tmp_path\"]\n                    )\n                    \n                    mock_detector = test_manager._setup_capability_detector(mocker)\n                    \n                    results = list(\n                        run_vfi_mod.run_vfi(\n                            folder=success_workspace[\"tmp_path\"],\n                            output_mp4_path=success_workspace[\"output_mp4\"],\n                            rife_exe_path=success_workspace[\"rife_exe\"],\n                            fps=10,\n                            num_intermediate_frames=1,\n                            max_workers=1,\n                            **success_config[\"kwargs\"]\n                        )\n                    )\n                    \n                    # If we get here, recovery was successful\n                    successful_recoveries += 1\n            \n            except Exception:\n                # Recovery failed\n                pass\n        \n        return {\n            \"success\": True,\n            \"recovery_tests\": recovery_tests,\n            \"successful_recoveries\": successful_recoveries,\n            \"recovery_rate\": successful_recoveries / recovery_tests if recovery_tests > 0 else 0,\n        }