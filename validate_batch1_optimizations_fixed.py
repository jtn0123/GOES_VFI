#!/usr/bin/env python3
"""
Validate Batch 1 test optimizations maintain 100% coverage (fixed version).
"""

import ast
import sys
from pathlib import Path
from typing import Dict, List, Set


def count_test_methods_in_class(class_node: ast.ClassDef) -> List[str]:
    """Count test methods in a class node."""
    methods = []
    for node in class_node.body:
        if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
            methods.append(f"{class_node.name}.{node.name}")
        elif isinstance(node, ast.AsyncFunctionDef) and node.name.startswith('test_'):
            methods.append(f"{class_node.name}.{node.name}")
    return methods


def analyze_test_file(filepath: Path) -> Dict:
    """Analyze a test file and extract metrics."""
    with open(filepath, 'r') as f:
        content = f.read()

    tree = ast.parse(content)

    test_classes = []
    test_methods = []
    test_functions = []
    parameterized_tests = []

    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            if 'Test' in node.name:  # More flexible class name matching
                test_classes.append(node.name)
                # Count methods in this class
                class_methods = count_test_methods_in_class(node)
                test_methods.extend(class_methods)

                # Check for parametrize decorators
                for item in node.body:
                    if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)) and item.name.startswith('test_'):
                        for decorator in item.decorator_list:
                            if isinstance(decorator, ast.Attribute) and decorator.attr == 'parametrize':
                                parameterized_tests.append(f"{node.name}.{item.name}")
                            elif isinstance(decorator, ast.Call) and hasattr(decorator.func, 'attr') and decorator.func.attr == 'parametrize':
                                parameterized_tests.append(f"{node.name}.{item.name}")

        elif isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
            # Top-level test function (not in a class)
            if not any(f".{node.name}" in m for m in test_methods):
                test_functions.append(node.name)

    # For parameterized tests, count the actual test cases
    param_expansion = 0
    if parameterized_tests:
        # Rough estimate: each parameterized test generates at least 2 test cases
        param_expansion = len(parameterized_tests) * 2

    return {
        'classes': len(test_classes),
        'methods': len(test_methods),
        'functions': len(test_functions),
        'total_tests': len(test_methods) + len(test_functions),
        'parameterized': len(parameterized_tests),
        'param_expansion': param_expansion,
        'effective_tests': len(test_methods) + len(test_functions) + param_expansion,
        'test_names': sorted(test_methods + test_functions),
    }


def compare_coverage(original: Dict, optimized: Dict, name: str) -> bool:
    """Compare coverage between original and optimized versions."""
    print(f"\n{'='*60}")
    print(f"üìä {name}")
    print('='*60)

    print(f"Original: {original['total_tests']} tests ({original['classes']} classes, {original['parameterized']} parameterized)")
    if original['param_expansion'] > 0:
        print(f"  Effective with parameterization: ~{original['effective_tests']} tests")

    print(f"Optimized: {optimized['total_tests']} tests ({optimized['classes']} classes, {optimized['parameterized']} parameterized)")
    if optimized['param_expansion'] > 0:
        print(f"  Effective with parameterization: ~{optimized['effective_tests']} tests")

    # Calculate coverage based on effective tests if parameterized
    if original['parameterized'] > 0 or optimized['parameterized'] > 0:
        coverage = (optimized['effective_tests'] / original['effective_tests'] * 100) if original['effective_tests'] > 0 else 0
    else:
        coverage = (optimized['total_tests'] / original['total_tests'] * 100) if original['total_tests'] > 0 else 0

    print(f"Coverage: {coverage:.1f}%")

    # Check test method mapping
    original_set = set(original['test_names'])
    optimized_set = set(optimized['test_names'])

    # For v2 tests, we expect different names but same functionality
    if '_v2' in str(optimized_file):
        print("Note: v2 tests use different class names but cover same functionality")

    # Success if 100% or better coverage
    success = coverage >= 100
    if success:
        print(f"\n‚úÖ Coverage maintained at {coverage:.1f}%")
    else:
        print(f"\n‚ùå Coverage below 100% threshold")

    return success


def main():
    """Validate Batch 1 optimizations."""
    print("üöÄ Batch 1 Test Optimization Validation (Fixed)")
    print("=" * 80)

    batch1_tests = [
        ("S3 Unsigned Access", "test_s3_unsigned_access.py", "test_s3_unsigned_access_v2.py"),
        ("Optimized Timeline Tab", "test_optimized_timeline_tab.py", "test_optimized_timeline_tab_v2.py"),
        ("Processing View Model FFmpeg", "test_processing_view_model_ffmpeg.py", "test_processing_view_model_ffmpeg_v2.py"),
        ("RIFE Analyzer", "test_rife_analyzer.py", "test_rife_analyzer_v2.py"),
        ("Date Utils", "test_date_utils.py", "test_date_utils_v2.py"),
    ]

    all_success = True
    results = []

    for name, original_file, optimized_file in batch1_tests:
        original_path = Path("tests/unit") / original_file
        optimized_path = Path("tests/unit") / optimized_file

        if not original_path.exists():
            print(f"\n‚ùå Original not found: {original_path}")
            continue

        if not optimized_path.exists():
            print(f"\n‚ùå Optimized not found: {optimized_path}")
            continue

        original_analysis = analyze_test_file(original_path)
        optimized_analysis = analyze_test_file(optimized_path)

        success = compare_coverage(original_analysis, optimized_analysis, name)
        all_success = all_success and success

        results.append({
            'name': name,
            'original_tests': original_analysis['total_tests'],
            'optimized_tests': optimized_analysis['total_tests'],
            'original_effective': original_analysis['effective_tests'],
            'optimized_effective': optimized_analysis['effective_tests'],
            'coverage': (optimized_analysis['effective_tests'] / original_analysis['effective_tests'] * 100) if original_analysis['effective_tests'] > 0 else 0,
            'success': success
        })

    # Summary
    print("\n" + "="*80)
    print("üìà BATCH 1 SUMMARY")
    print("="*80)

    total_original = sum(r['original_tests'] for r in results)
    total_optimized = sum(r['optimized_tests'] for r in results)
    total_original_eff = sum(r['original_effective'] for r in results)
    total_optimized_eff = sum(r['optimized_effective'] for r in results)

    print(f"\nTotal original tests: {total_original} (effective: {total_original_eff})")
    print(f"Total optimized tests: {total_optimized} (effective: {total_optimized_eff})")
    print(f"Overall coverage: {total_optimized_eff/total_original_eff*100:.1f}%")

    print("\nIndividual results:")
    for r in results:
        status = "‚úÖ" if r['success'] else "‚ùå"
        print(f"  {status} {r['name']}: {r['coverage']:.1f}% ({r['optimized_effective']}/{r['original_effective']} effective tests)")

    if all_success:
        print("\n‚úÖ All Batch 1 tests maintain 100% or better coverage!")
    else:
        print("\n‚ùå Some tests need improvement to reach 100% coverage")

    return 0 if all_success else 1


if __name__ == "__main__":
    sys.exit(main())